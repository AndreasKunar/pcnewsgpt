#common to both .py progs
PERSIST_DIRECTORY=db
COLLECTION_NAME=langchain
EMBEDDINGS_MODEL_NAME=intfloat/multilingual-e5-large 

#ingest.py
SOURCE_DIRECTORY=source_documents
APPEND_DIRECTORY=append_documents
TEXT_SPLITTER=SpacyTextSplitter
TEXT_SPLITTER_PARAMETERS={'chunk_size':1000,'chunk_overlap':0,'pipeline':'de_core_news_lg'}

#query.py
# llama tuned for German: https://huggingface.co/flozi00/Llama-2-13b-german-assistant-v7/tree/main 
# but this needs quantization and conversion to gguf via llama.cpp's conversion scripts
# MODEL_PATH=./models/llama2-13b-german-assistant-V7.Q4_0.gguf
# tuned Mistral is better: download model from https://huggingface.co/TheBloke/em_german_leo_mistral-GGUF
# MODEL_PATH=./models/em_german_leo_mistral.Q5_K_M.gguf
# yet anothermultilingual alternative is mistral "tiny" v0.2 https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF
MODEL_PATH=./models/mistral-7b-instrict-v0.2.Q5_K_M.gguf
MODEL_N_CTX=4096
MODEL_TEMP=0.05
MAX_TOKENS=-2
# 3 items below need edits for different hosts/models
# number of CPU threads to use when running on CPU - 4 for Apple M2
MODEL_THREADS=4
# number of GPU-layers, depends on model
MODEL_GPU=46
# use "print details" below and use prompt eval time tokens/s
MODEL_PROMPT_PER_S=38
# context settings
MAX_CONTEXT_CHUNKS=6
MAX_CONTEXT_DISTANCE=0.434
MAX_RESORT_DISTANCE=0.08
# print details for debugging
HIDE_SOURCE=True
HIDE_SOURCE_DETAILS=True
